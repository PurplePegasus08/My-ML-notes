Machine learning only started to flourish in the 1990s, it has quickly become the most popular and most successful subfield of AI, a trend driven by theavailability of faster hardware and larger datasets. Machine learning is tightly related to mathematical statistics, but it differs from statistics in several important ways.Unlike statistics, machine learning tends to deal with large, complex datasets (such asa dataset of millions of images, each consisting of tens of thousands of pixels) forwhich classical statistical analysis such as Bayesian analysis would be impractical.Machine-learning algorithms aren’t usually creative in they’re merely searching through a predefined set of operations, called a hypothesis space.


Deep Learning (DL) - Deep learning is a specific subfield of machine learning: a new take on learning repre-sentations from data that puts an emphasis on learning successive layers of increasingly meaningful representations.How many layers contribute to a model of the data is called the depth of the model. Other appropriate names for the field could have been layered representations learning and hierarchical representations learning and they’re all learned automatically from exposure to training data.


Meanwhile,
other approaches to machine learning tend to focus on learning only one or two lay- ers of representations of the data; hence, they’re sometimes called shallow learning.

In deep learning, these layered representations are (almost always) learned via models called neural networks, structured in literal layers stacked on top of each other.

Understanding how deep learning works, in three figures

The specification of what a layer does to its input data is stored in the layer’s weights, which in essence are a bunch of numbers. In technical terms, we’d say that the transformation implemented by a layer is parameterized by its weights (Weights are also sometimes called the parameters of a layer.). In this context, learning means finding a set of values for the weights of all layers in a network, such that the network will correctly map example inputs to their associated targets.

 Loss function : To control the output of a neural network, you need to be able to measure how far this output is from what you expected. This is the job of the loss function of the network, also called the objective function. The loss function takes the predictions of the network and the true target (what you wanted the network to output) and computes a distance score, capturing how well the network has done on this specific example. The fundamental trick in deep learning is to use this score as a feedback signal to adjust the value of the weights a little, in a direction that will lower the loss score for the current example  . This adjustment is the job of the optimizer, which implements what’s called the Backpropagation algorithm: the central algorithm in deep learning.


 Initially, the weights of the network are assigned random values, so the network merely implements a series of random transformations. Naturally, its output is far from what it should ideally be, and the loss score is accordingly very high. But with every example the network processes, the weights are adjusted a little in the correct direction, and the loss score decreases. This is the training loop, which, repeated a suffi-cient number of times (typically tens of iterations over thousands of examples), yields weight values that minimize the loss function.